# Story 3.2: AI Prioritization Service

## Status
Draft

## Story
**As a** Developer,
**I want** a backend service that can score work items based on their alignment with our defined project goals,
**so that** we have a foundation for the AI prioritization feature.

## Acceptance Criteria
1. **Service/Module Creation**: Backend service/module is created with proper FastAPI integration and follows the Modular Monolith pattern [Source: docs/architecture/10-backend-architecture.md]
2. **Goal-Based Scoring Algorithm**: Service scores work items based on keyword matching against project goals, using text preprocessing pipeline established in Story 3.1
3. **Prioritized List Endpoint**: Service exposes REST endpoint that returns a prioritized list of work items with scoring metadata
4. **Performance Requirements**: API response time must be < 500ms for teams with up to 1000 work items
5. **Unit Testing**: Service has comprehensive unit tests with minimum 80% coverage as per coding standards [Source: docs/architecture/coding-standards.md]

## Tasks / Subtasks
- [ ] Design scoring algorithm architecture (AC: 2)
  - [ ] Research keyword matching algorithms (TF-IDF, cosine similarity)
  - [ ] Define scoring formula incorporating priority weight from project goals
  - [ ] Create algorithm specification document
- [ ] Implement backend service module (AC: 1, 2)
  - [ ] Create `app/domains/services/ai_prioritization_service.py` following Repository Pattern
  - [ ] Implement keyword extraction and text preprocessing functions
  - [ ] Build scoring algorithm with configurable weights
  - [ ] Add proper dependency injection using FastAPI Depends
- [ ] Create REST API endpoint (AC: 3)
  - [ ] Add new router `app/api/v1/endpoints/ai_prioritization.py`
  - [ ] Implement `POST /teams/{teamId}/ai-prioritization/score` endpoint
  - [ ] Add Pydantic schemas for request/response validation
  - [ ] Follow authentication pattern using secure HTTP-only cookies
- [ ] Implement performance optimizations (AC: 4)
  - [ ] Add Redis caching for project goals and scoring results
  - [ ] Optimize database queries with proper indexing
  - [ ] Add performance monitoring with response time metrics
- [ ] Comprehensive testing suite (AC: 5)
  - [ ] Unit tests for scoring algorithm with various scenarios
  - [ ] Integration tests for API endpoint with authentication
  - [ ] Performance tests to validate < 500ms requirement
  - [ ] Test coverage verification using pytest-cov

## Dev Notes

### Business Context & Algorithm Design
[Source: docs/prd/6-epic-details.md#Epic3Story2 + Story 3.1 Implementation Insights]

**Core Scoring Algorithm Requirements:**
- **Text Matching**: Use keyword extraction from work item titles/descriptions against project goal descriptions
- **Priority Weighting**: Incorporate priority_weight field (1-10) from project goals as multiplication factor
- **Scoring Range**: Output scores from 0.0 to 10.0 for consistent ranking
- **Tie-Breaking**: Use work item priority field as secondary sort criteria

**Text Processing Pipeline (from Story 3.1):**
1. HTML stripping for rich text goal descriptions
2. Text normalization (lowercase, whitespace cleanup)
3. Keyword extraction (meaningful terms, domain-specific preservation)
4. Stop word removal and stemming for better matching
5. Minimum 10 character requirement for meaningful analysis

**Algorithm Specification:**
```
Base Score = (Keyword Match Count / Total Goal Keywords) * Goal Priority Weight
Final Score = Base Score + (Work Item Priority * 0.1) // Tie-breaker component
```

### Architecture Context
[Source: docs/architecture/10-backend-architecture.md + docs/architecture/3-tech-stack.md]

**Backend Service Pattern:**
- **Framework**: FastAPI ~0.109.0 with automatic OpenAPI documentation
- **Architecture**: Modular Monolith pattern with feature-based router grouping
- **Database**: PostgreSQL 16.2 with Repository Pattern for all data access
- **Caching**: Redis 7.2 for performance optimization of goal data and results

**Service Layer Structure:**
```
app/domains/services/ai_prioritization_service.py  # Core business logic
app/api/v1/endpoints/ai_prioritization.py          # REST API endpoints
app/domains/schemas/ai_prioritization.py           # Pydantic request/response schemas
app/domains/repositories/ai_prioritization_repo.py # Data access layer
```

### Database Schema Requirements
[Source: Story 3.1 ProjectGoal model + docs/architecture/4-data-models.md patterns]

**Required Tables Access:**
- `project_goals`: For goal descriptions and priority weights
- `work_items`: For item titles, descriptions, and current priorities
- `teams`: For team association and access control

**New Caching Schema (Redis):**
```
Key Pattern: "ai_priority:goals:{team_id}"
Value: JSON array of preprocessed goal data with keywords
TTL: 1 hour (goals change infrequently)

Key Pattern: "ai_priority:scores:{team_id}:{hash_of_work_items}"
Value: JSON array of scored work item results
TTL: 10 minutes (work items change frequently)
```

### API Specifications
[Source: docs/architecture/5-api-specification.md + Story 3.1 established patterns]

**New Endpoint Design:**
```
POST /api/v1/teams/{teamId}/ai-prioritization/score
Headers:
  - Content-Type: application/json
  - Cookie: session_token (secure, HTTP-only)
Request Body: {
  "work_item_ids": ["uuid1", "uuid2", ...],  // Optional: specific items to score
  "include_metadata": boolean                 // Optional: include scoring details
}
Response: {
  "scored_items": [
    {
      "work_item_id": "uuid",
      "title": "string",
      "current_priority": number,
      "ai_score": number,
      "suggested_rank": number,
      "scoring_metadata": {  // If include_metadata=true
        "matched_goals": ["goal_id1", "goal_id2"],
        "keyword_matches": ["keyword1", "keyword2"],
        "base_score": number,
        "priority_adjustment": number
      }
    }
  ],
  "total_items": number,
  "generation_time_ms": number
}
```

**Authentication & Authorization:**
- Follow established pattern from Story 3.1: secure HTTP-only cookies
- User must be team member to access scoring service
- Same role-based access: all team members can request scoring

### Performance Requirements
[Source: AC #4 + docs/architecture/coding-standards.md testing requirements]

**Response Time Targets:**
- **< 500ms** for teams with up to 1000 work items
- **< 100ms** for cached results (Redis hit)
- **< 200ms** for goal preprocessing (first-time team scoring)

**Performance Optimization Strategies:**
- Redis caching of preprocessed project goals (TTL: 1 hour)
- Database query optimization with proper indexes on team_id, work_item status
- Lazy loading: only process goals and work items when scoring requested
- Batch processing: score all team items in single request to minimize API calls

**Monitoring Integration:**
- OpenTelemetry instrumentation for response times
- Custom metrics for cache hit/miss ratios
- Error rate tracking for scoring algorithm failures

### Testing Standards
[Source: docs/architecture/coding-standards.md#Testing Requirements]

**Backend Testing Requirements:**
- **Minimum Coverage**: 80% (pytest-cov)
- **Test File Location**: `backend/tests/unit/services/test_ai_prioritization_service.py`
- **Integration Tests**: `backend/tests/integration/test_ai_prioritization_api.py`

**Testing Framework Stack:**
- pytest for test runner with asyncio support
- httpx AsyncClient for API endpoint testing  
- pytest-mock for mocking external dependencies
- pytest-cov for coverage reporting

**Test Scenarios to Cover:**
1. **Algorithm Tests**: Various keyword matching scenarios, edge cases
2. **Performance Tests**: Response time validation with different data sizes
3. **Authentication Tests**: Team member access control
4. **Error Handling**: Invalid team IDs, missing goals, malformed requests
5. **Cache Tests**: Redis integration, cache invalidation scenarios

### Project Structure Alignment
[Source: docs/architecture/11-unified-project-structure.md implied patterns]

**File Locations (following established patterns from existing codebase):**
```
backend/app/domains/services/ai_prioritization_service.py
backend/app/domains/schemas/ai_prioritization.py  
backend/app/domains/repositories/ai_prioritization_repo.py (if needed)
backend/app/api/v1/endpoints/ai_prioritization.py
backend/tests/unit/services/test_ai_prioritization_service.py
backend/tests/integration/test_ai_prioritization_api.py
```

**Dependency Management:**
- Use Poetry for Python dependency management (as per project rules)
- Add any new ML/text processing libraries to `backend/pyproject.toml`
- Consider lightweight libraries: scikit-learn for TF-IDF, or custom implementation

### Integration with Story 3.1 Artifacts
**Project Goals Data Model Usage:**
- Leverage `project_goals.description` field for keyword analysis
- Use `project_goals.priority_weight` (1-10) as scoring multiplier
- Access via existing team association and role-based permissions

**Rich Text Processing Pipeline:**
- Reuse HTML stripping logic from Story 3.1 implementation
- Apply same text normalization and keyword extraction methods
- Maintain consistency in preprocessing between goal creation and scoring

### Constraints & Requirements
**Project Rules Compliance:**
- ✅ Local Supabase instance usage (not applicable - using PostgreSQL directly)
- ✅ Poetry for Python dependency management
- ✅ Coding standards adherence with `claude_suggestions.md` cross-reference
- ✅ No CI/CD workflow until QA approval (status remains Draft)
- ✅ Changelog updates for any bmad documentation modifications

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-20 | 1.0 | Initial story creation with comprehensive context | SM (Bob) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled during implementation*

### Debug Log References
*To be filled during implementation*

### Completion Notes List
*To be filled during implementation*

### File List
*To be filled during implementation*

## QA Results
*Results from QA Agent review of the completed story implementation*
