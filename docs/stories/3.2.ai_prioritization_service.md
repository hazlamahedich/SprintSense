# Story 3.2: AI Prioritization Service

## Status
Approved

## Story
**As a** Developer,
**I want** a backend service that can score work items based on their alignment with our defined project goals,
**so that** we have a foundation for the AI prioritization feature.

## Acceptance Criteria
1. **Service/Module Creation**: Backend service/module is created with proper FastAPI integration and follows the Modular Monolith pattern [Source: docs/architecture/10-backend-architecture.md]
2. **Goal-Based Scoring Algorithm**: Service scores work items based on keyword matching against project goals, using text preprocessing pipeline established in Story 3.1
3. **Prioritized List Endpoint**: Service exposes REST endpoint that returns a prioritized list of work items with scoring metadata
4. **Performance Requirements**: API response time must be < 500ms for teams with up to 1000 work items
5. **Unit Testing**: Service has comprehensive unit tests with minimum 80% coverage as per coding standards [Source: docs/architecture/coding-standards.md]
6. **Business Value Validation**: Service demonstrates measurable improvement in prioritization accuracy through A/B testing framework preparation [Added by PO Review]
7. **Workflow Integration Ready**: API design supports seamless integration with Stories 3.3 and 3.4 user workflows (suggestion review, epic clustering) [Added by PO Review]
8. **Error Handling & Resilience**: Service handles all failure scenarios gracefully with appropriate error responses and fallback behaviors [Added by QA Review]
9. **Performance Testing Validation**: Comprehensive load testing validates response times under realistic concurrent usage scenarios [Added by QA Review]

## Tasks / Subtasks
- [ ] Design scoring algorithm architecture (AC: 2)
  - [ ] Research keyword matching algorithms (TF-IDF, cosine similarity)
  - [ ] Define scoring formula incorporating priority weight from project goals
  - [ ] Create algorithm specification document
- [ ] Implement backend service module (AC: 1, 2)
  - [ ] Create `app/domains/services/ai_prioritization_service.py` following Repository Pattern
  - [ ] Implement keyword extraction and text preprocessing functions
  - [ ] Build scoring algorithm with configurable weights
  - [ ] Add proper dependency injection using FastAPI Depends
- [ ] Create REST API endpoint (AC: 3)
  - [ ] Add new router `app/api/v1/endpoints/ai_prioritization.py`
  - [ ] Implement `POST /teams/{teamId}/ai-prioritization/score` endpoint
  - [ ] Add Pydantic schemas for request/response validation
  - [ ] Follow authentication pattern using secure HTTP-only cookies
- [ ] Implement performance optimizations (AC: 4)
  - [ ] Add Redis caching for project goals and scoring results
  - [ ] Optimize database queries with proper indexing
  - [ ] Add performance monitoring with response time metrics
- [ ] Comprehensive testing suite (AC: 5)
  - [ ] Unit tests for scoring algorithm with various scenarios
  - [ ] Integration tests for API endpoint with authentication
  - [ ] Performance tests to validate < 500ms requirement
  - [ ] Test coverage verification using pytest-cov
- [ ] Business value validation framework (AC: 6)
  - [ ] Add scoring accuracy metrics collection
  - [ ] Implement baseline comparison capability
  - [ ] Create A/B testing hooks for future validation
  - [ ] Document success criteria for prioritization improvement
- [ ] Workflow integration preparation (AC: 7)
  - [ ] Design API to support single-suggestion retrieval (Story 3.3)
  - [ ] Add clustering-friendly scoring metadata (Story 3.4)
  - [ ] Ensure response format supports UI integration patterns
  - [ ] Add explanation/reasoning fields for user transparency
- [ ] Error handling & resilience implementation (AC: 8)
  - [ ] Handle Redis cache unavailability with database fallback
  - [ ] Implement graceful degradation for empty goals/work items
  - [ ] Add input validation for malformed requests and data
  - [ ] Create proper HTTP error responses (400, 404, 500, 503)
  - [ ] Add circuit breaker pattern for external dependencies
  - [ ] Implement request timeout handling and cleanup
- [ ] Performance testing & validation (AC: 9)
  - [ ] Create load testing scenarios: 1/10/50 concurrent users
  - [ ] Test with varying data sizes: 10/100/1000 work items
  - [ ] Validate cache performance with hit/miss scenarios
  - [ ] Memory usage profiling and leak detection
  - [ ] Database connection pool stress testing
  - [ ] Response time distribution analysis (p95, p99 percentiles)

## Dev Notes

### Business Context & Algorithm Design
[Source: docs/prd/6-epic-details.md#Epic3Story2 + Story 3.1 Implementation Insights]

**Business Value Proposition:**
- **Problem**: Teams struggle with manual prioritization leading to suboptimal backlog ordering
- **Solution**: AI-driven scoring based on strategic goal alignment provides objective prioritization foundation
- **Success Metrics**: Reduce time-to-prioritize by 60%, increase goal alignment accuracy by 40%
- **User Journey Integration**: Enables Stories 3.3 (Review/Apply Suggestions) and 3.4 (Epic Clustering)

**Core Scoring Algorithm Requirements:**
- **Text Matching**: Use keyword extraction from work item titles/descriptions against project goal descriptions
- **Priority Weighting**: Incorporate priority_weight field (1-10) from project goals as multiplication factor
- **Scoring Range**: Output scores from 0.0 to 10.0 for consistent ranking
- **Tie-Breaking**: Use work item priority field as secondary sort criteria
- **Transparency**: Include explainability data for user trust and decision validation

**Text Processing Pipeline (from Story 3.1):**
1. HTML stripping for rich text goal descriptions
2. Text normalization (lowercase, whitespace cleanup)
3. Keyword extraction (meaningful terms, domain-specific preservation)
4. Stop word removal and stemming for better matching
5. Minimum 10 character requirement for meaningful analysis

**Algorithm Specification (FIXED by QA Review):**
```
// Defensive normalization with bounds checking
if Total_Goal_Keywords == 0: Base_Score = 0
else: Base_Score = min(10.0, (Keyword_Match_Count / Total_Goal_Keywords) * Goal_Priority_Weight)

// Work item priority normalized to 0-1 scale (assumes 0-10 range, configurable)
Priority_Adjustment = (Work_Item_Priority / 10.0) * 0.5  // Max 0.5 points adjustment

// Final score bounded to specified range
Final_Score = min(10.0, Base_Score + Priority_Adjustment)

// Error Handling:
// - Division by zero protection
// - Score range enforcement (0.0 - 10.0)
// - Configurable priority scale normalization
```

### Architecture Context
[Source: docs/architecture/10-backend-architecture.md + docs/architecture/3-tech-stack.md]

**Backend Service Pattern:**
- **Framework**: FastAPI ~0.109.0 with automatic OpenAPI documentation
- **Architecture**: Modular Monolith pattern with feature-based router grouping
- **Database**: PostgreSQL 16.2 with Repository Pattern for all data access
- **Caching**: Redis 7.2 for performance optimization of goal data and results

**Service Layer Structure:**
```
app/domains/services/ai_prioritization_service.py  # Core business logic
app/api/v1/endpoints/ai_prioritization.py          # REST API endpoints
app/domains/schemas/ai_prioritization.py           # Pydantic request/response schemas
app/domains/repositories/ai_prioritization_repo.py # Data access layer
```

### Database Schema Requirements
[Source: Story 3.1 ProjectGoal model + docs/architecture/4-data-models.md patterns]

**Required Tables Access:**
- `project_goals`: For goal descriptions and priority weights
- `work_items`: For item titles, descriptions, and current priorities
- `teams`: For team association and access control

**New Caching Schema (Redis):**
```
Key Pattern: "ai_priority:goals:{team_id}"
Value: JSON array of preprocessed goal data with keywords
TTL: 1 hour (goals change infrequently)

Key Pattern: "ai_priority:scores:{team_id}:{hash_of_work_items}"
Value: JSON array of scored work item results
TTL: 10 minutes (work items change frequently)
```

### API Specifications
[Source: docs/architecture/5-api-specification.md + Story 3.1 established patterns]

**New Endpoint Design:**
```
POST /api/v1/teams/{teamId}/ai-prioritization/score
Headers:
  - Content-Type: application/json
  - Cookie: session_token (secure, HTTP-only)
Request Body: {
  "work_item_ids": ["uuid1", "uuid2", ...],  // Optional: specific items to score
  "include_metadata": boolean,                // Optional: include scoring details
  "mode": "full" | "suggestion" | "clustering" // Added: Support different workflow modes
}
Response: {
  "scored_items": [
    {
      "work_item_id": "uuid",
      "title": "string",
      "current_priority": number,
      "ai_score": number,
      "suggested_rank": number,
      "confidence_level": "high" | "medium" | "low", // Added: User confidence indicator
      "explanation": "string",                        // Added: Human-readable reasoning
      "scoring_metadata": {  // If include_metadata=true
        "matched_goals": [
          {
            "goal_id": "uuid",
            "goal_title": "string",
            "match_strength": number,
            "matched_keywords": ["keyword1", "keyword2"]
          }
        ],
        "base_score": number,
        "priority_adjustment": number,
        "clustering_similarity": number  // Added: For Story 3.4 epic clustering
      }
    }
  ],
  "total_items": number,
  "generation_time_ms": number,
  "business_metrics": {  // Added: For business value validation
    "accuracy_score": number,
    "coverage_percentage": number,
    "algorithm_version": "string"
  }
}
```

**Authentication & Authorization:**
- Follow established pattern from Story 3.1: secure HTTP-only cookies
- User must be team member to access scoring service
- Same role-based access: all team members can request scoring

### Performance Requirements
[Source: AC #4 + docs/architecture/coding-standards.md testing requirements]

**Response Time Targets:**
- **< 500ms** for teams with up to 1000 work items
- **< 100ms** for cached results (Redis hit)
- **< 200ms** for goal preprocessing (first-time team scoring)

**Performance Optimization Strategies:**
- Redis caching of preprocessed project goals (TTL: 1 hour)
- Database query optimization with proper indexes on team_id, work_item status
- Lazy loading: only process goals and work items when scoring requested
- Batch processing: score all team items in single request to minimize API calls

**Monitoring Integration:**
- OpenTelemetry instrumentation for response times
- Custom metrics for cache hit/miss ratios
- Error rate tracking for scoring algorithm failures

### Error Handling Specifications
[Added by QA Review - Critical for Production Readiness]

**Service Failure Scenarios & Responses:**

| Scenario | HTTP Status | Response Body | Fallback Behavior |
|----------|-------------|---------------|------------------|
| Redis Unavailable | 200 | Normal response | Use database-only mode, log warning |
| PostgreSQL Timeout | 503 | `{"error": "service_unavailable", "retry_after": 30}` | Return cached results if available |
| No Goals Defined | 200 | `{"scored_items": [], "warning": "no_goals_configured"}` | Return empty results with guidance |
| Malformed Work Items | 400 | `{"error": "invalid_work_items", "details": [...]}` | Skip invalid items, process valid ones |
| Text Processing Failure | 200 | Reduced metadata | Use basic title matching fallback |
| Algorithm Division by Zero | 200 | Score = 0.0 | Graceful degradation with logging |
| Memory Exhaustion | 503 | `{"error": "resource_limit", "retry_after": 60}` | Trigger circuit breaker |
| Concurrent Request Limit | 429 | `{"error": "rate_limit", "retry_after": 10}` | Queue or reject with backoff |

**Circuit Breaker Configuration:**
- **Failure Threshold**: 5 consecutive failures
- **Recovery Timeout**: 30 seconds  
- **Half-Open Test**: Single request probe
- **Metrics**: Track breaker state changes

### Testing Standards
[Source: docs/architecture/coding-standards.md#Testing Requirements]

**Backend Testing Requirements:**
- **Minimum Coverage**: 80% (pytest-cov)
- **Test File Location**: `backend/tests/unit/services/test_ai_prioritization_service.py`
- **Integration Tests**: `backend/tests/integration/test_ai_prioritization_api.py`

**Testing Framework Stack:**
- pytest for test runner with asyncio support
- httpx AsyncClient for API endpoint testing  
- pytest-mock for mocking external dependencies
- pytest-cov for coverage reporting

**Test Scenarios to Cover (Enhanced by QA Review):**
1. **Algorithm Tests**:
   - Various keyword matching scenarios (exact, partial, stemmed matches)
   - Edge cases: empty goals, no keywords, special characters
   - Mathematical edge cases: division by zero, score boundaries
   - Multi-goal scoring combinations and priority weighting
2. **Performance Tests**:
   - Response time validation: 10/100/1000 work items
   - Concurrent user scenarios: 1/10/50 simultaneous requests
   - Cache performance: cold start vs warm cache scenarios
   - Memory usage patterns and garbage collection impact
3. **Authentication Tests**:
   - Team member access control with different roles
   - Session expiration and refresh scenarios
   - Cross-team access prevention
4. **Error Handling Tests**:
   - Database connectivity issues (timeout, connection pool exhaustion)
   - Redis cache failures and fallback behavior
   - Malformed input data and boundary conditions
   - Resource exhaustion and circuit breaker activation
5. **Cache Tests**:
   - Redis integration with different TTL scenarios
   - Cache invalidation on goal/work item updates
   - Cache consistency during concurrent modifications
6. **Integration Tests**:
   - End-to-end workflow: goal creation â†’ scoring â†’ result validation
   - Cross-service dependencies and failure propagation
   - API contract validation with OpenAPI schema

### Project Structure Alignment
[Source: docs/architecture/11-unified-project-structure.md implied patterns]

**File Locations (following established patterns from existing codebase):**
```
backend/app/domains/services/ai_prioritization_service.py
backend/app/domains/schemas/ai_prioritization.py  
backend/app/domains/repositories/ai_prioritization_repo.py (if needed)
backend/app/api/v1/endpoints/ai_prioritization.py
backend/tests/unit/services/test_ai_prioritization_service.py
backend/tests/integration/test_ai_prioritization_api.py
```

**Dependency Management:**
- Use Poetry for Python dependency management (as per project rules)
- Add any new ML/text processing libraries to `backend/pyproject.toml`
- Consider lightweight libraries: scikit-learn for TF-IDF, or custom implementation

### Integration with Story 3.1 Artifacts
**Project Goals Data Model Usage:**
- Leverage `project_goals.description` field for keyword analysis
- Use `project_goals.priority_weight` (1-10) as scoring multiplier
- Access via existing team association and role-based permissions

**Rich Text Processing Pipeline:**
- Reuse HTML stripping logic from Story 3.1 implementation
- Apply same text normalization and keyword extraction methods
- Maintain consistency in preprocessing between goal creation and scoring

### Constraints & Requirements
**Project Rules Compliance:**
- âœ… Local Supabase instance usage (not applicable - using PostgreSQL directly)
- âœ… Poetry for Python dependency management
- âœ… Coding standards adherence with `claude_suggestions.md` cross-reference
- âœ… No CI/CD workflow until QA approval (status remains Draft)
- âœ… Changelog updates for any bmad documentation modifications

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-20 | 1.2 | QA Review - CRITICAL FIXES: Fixed algorithm mathematical flaws (division by zero, score bounds), added comprehensive error handling (AC 8,9), performance testing specifications, resilience patterns, and production readiness validation | QA (Quinn) |
| 2025-01-20 | 1.1 | PO Review enhancements: Added business value validation (AC 6,7), workflow integration readiness, API design improvements for Stories 3.3/3.4, success metrics, and user transparency features | PO (Sarah) |
| 2025-01-20 | 1.0 | Initial story creation with comprehensive context | SM (Bob) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled during implementation*

### Debug Log References
*To be filled during implementation*

### Completion Notes List
*To be filled during implementation*

### File List
*To be filled during implementation*

## QA Results

### QA Review Summary - Story 3.2: AI Prioritization Service
**Reviewer:** Quinn (Test Architect)  
**Review Date:** 2025-01-20  
**Review Status:** âœ… **APPROVED** (with critical fixes applied)

### Technical Assessment Results

#### **ðŸ”§ Critical Issues Found & Resolved:**
1. **Algorithm Mathematical Flaws** - FIXED
   - âŒ **Original**: Division by zero risk, unbounded scoring
   - âœ… **Fixed**: Added bounds checking, normalization, defensive programming
   - **Impact**: Prevents production crashes and ensures consistent 0.0-10.0 scoring range

2. **Missing Error Handling Specifications** - ADDED
   - âŒ **Gap**: No failure scenario coverage
   - âœ… **Added**: Comprehensive error handling matrix with fallback behaviors
   - **Impact**: Production-ready resilience patterns

3. **Performance Requirements Ambiguity** - CLARIFIED  
   - âŒ **Vague**: "< 500ms for 1000 items" (missing concurrency context)
   - âœ… **Detailed**: Specific load testing scenarios, percentile requirements
   - **Impact**: Testable, measurable performance criteria

#### **ðŸ“Š Quality Metrics Assessment:**
- **Test Coverage Target**: 80% âœ… (meets coding standards)
- **Performance Testability**: âœ… (comprehensive load testing scenarios)
- **Error Resilience**: âœ… (circuit breaker, graceful degradation)
- **API Contract Clarity**: âœ… (detailed request/response specifications)
- **Business Value Measurement**: âœ… (A/B testing framework included)

#### **ðŸš€ Production Readiness Checklist:**
- âœ… **Algorithm Robustness**: Mathematical edge cases handled
- âœ… **Error Handling**: All failure scenarios specified
- âœ… **Performance Testing**: Load testing strategy defined
- âœ… **Monitoring Integration**: OpenTelemetry, custom metrics
- âœ… **Security Compliance**: Authentication patterns established
- âœ… **Workflow Integration**: Stories 3.3/3.4 compatibility ensured

#### **ðŸ“ˆ Risk Assessment:**
- **Technical Risk**: LOW (critical issues resolved)
- **Performance Risk**: LOW (comprehensive testing strategy)
- **Integration Risk**: LOW (API design supports downstream stories)
- **Business Risk**: LOW (success metrics and validation framework included)

### QA Recommendation: âœ… **APPROVE FOR DEVELOPMENT**

This story is now technically sound and ready for implementation. The critical algorithm flaws have been resolved, comprehensive error handling has been specified, and the testing strategy will ensure production readiness. The enhanced API design properly supports the user workflows in subsequent stories.

**Next Steps:**
1. Development team can begin implementation with confidence
2. All acceptance criteria are testable and measurable
3. Performance benchmarks are clearly defined
4. Error scenarios are fully specified for robust implementation
